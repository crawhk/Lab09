---
title: "Lab 09: Algorithmic Bias"
author: "Hannah Crawley"
date: "4/9/2025"
output: github_document
---

## Load Packages and Data  

First, let's load the necessary packages:  

```{r load-packages, message = FALSE}
library(tidyverse)
library(fairness)
library(janitor)
```

### The data

For this lab, we'll use the COMPAS dataset compiled by ProPublica. The data has been preprocessed and cleaned for you. You'll have to load it yourself. The dataset is available in the `data` folder, but I've changed the file name from `compas-scores-two-years.csv` to `compas-scores-2-years.csv`. I've done this help you practice debugging code when you encounter an error.

```{r}
compas <- read.csv("data/compas-scores-2-years.csv")
```

# Part 1: Exploring the Data 

## Exercise 1
> What are the dimensions of the COMPAS dataset? (Hint: Use inline R code and functions like nrow and ncol to compose your answer.) What does each row in the dataset represent? What are the variables?

## Exercise 2
> How many unique defendants are in the dataset? Is this the same as the number of rows? If not, why might there be a difference?

## Exercise 3
> Let’s examine the distribution of the COMPAS risk scores (decile_score)! What do you observe about the shape of this distribution?

## Exercise 4
> Let’s examine the demographic distribution in our dataset. Create visualizations to show:
The distribution of defendants by race
The distribution of defendants by sex
The distribution of defendants by age category

> For an extra challenge, try to create a single visualization that shows all three 
distributions side by side. You can use facets or color to differentiate between the different demographic groups. 

# Part 2: Risk Scores and Recidivism 

## Exercise 5 
> Create a visualization showing the relationship between risk scores (decile_score) and actual recidivism (two_year_recid). Do higher risk scores actually correspond to higher rates of recidivism?

## Exercise 6
> Calculate the overall accuracy of the COMPAS algorithm. For this exercise, consider a prediction “correct” if:
A defendant with a high risk score (decile_score >= 7) did recidivate (two_year_recid = 1)
A defendant with a low risk score (decile_score <= 4) did not recidivate (two_year_recid = 0)

## Exercise 7 
> How well does the COMPAS algorithm perform overall? What percentage of its predictions are correct based on your calculation above?

# Part 3: Investigating disparities 

Now let’s assess the predictive accuracy of the COMPAS algorithm across different demographic groups. For this exercise, we’ll focus on race, but you can also explore other demographic variable

## Exercise 8
> Create visualizations comparing the distribution of risk scores (decile_score) between Black and white defendants. Do you observe any differences?

## Exercise 9 
> Calculate the percentage of Black defendants and white defendants who were classified as high risk (decile_score >= 7). Is there a disparity?

## Exercise 10
> Now, let’s look at the accuracy of predictions for different racial groups. Calculate the following metrics separately for Black defendants and white defendants:
False Positive Rate: Proportion of non-recidivists (two_year_recid = 0) who were classified as high risk (decile_score >= 7)
False Negative Rate: Proportion of recidivists (two_year_recid = 1) who were classified as low risk (decile_score <= 4)

## Exercise 11 
> Create a visualization comparing these metrics between Black and white defendants. What disparities do you observe?

# Part 4: Understanding the source of bias 

Note that their are many ways to measure bias in an algorithm. In this exercise, we’ll focus on disparities in the false positive and false negative rates. You can also explore other measures of bias, including in the stretch goals.


## Exercise 12
> Let’s investigate what factors might be contributing to the disparities we’ve observed. Create a visualization showing the relationship between prior convictions (priors_count) and risk score (decile_score), colored by race. Does the algorithm weigh prior convictions differently for different racial groups?

## Exercise 13
> In 2016, ProPublica and Northpointe (the company that created COMPAS) had a disagreement about how to measure fairness. ProPublica focused on error rates (false positives and false negatives), while Northpointe focused on calibration (whether the same score means the same probability of recidivism across groups). Based on your analysis, do you see evidence supporting ProPublica’s claim that the algorithm is biased? Explain your reasoning.


# Part 5: Designing fairer algorithms 

## Exercise 14
> If you were tasked with creating a fairer risk assessment algorithm, what changes would you make to address the disparities you’ve observed?

## Exercise 15
> Different definitions of fairness can sometimes be mathematically incompatible with each other. What trade-offs might be involved in designing a “fair” algorithm for criminal risk assessment?

## Exercise 16
> Beyond technical solutions, what policy changes might be needed to ensure that algorithmic risk assessments are used fairly in the criminal justice system?

